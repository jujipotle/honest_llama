{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import yaml\n",
    "# from omegaconf import OmegaConf\n",
    "import glob\n",
    "import types\n",
    "# from q_probe import sweep_utils, rl_networks\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import pyvene as pv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "# from bigcode_eval import tasks\n",
    "import llama\n",
    "\n",
    "from pprint import pprint\n",
    "def ppprint(code):\n",
    "    print(highlight(code, PythonLexer(), TerminalFormatter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylfs06/LABS/kempner_undergrads/Users/jujipotle/envs/iti_h100/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:519: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "          (att_out): Identity()\n",
      "          (value_out): Identity()\n",
      "          (head_out): Identity()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"baffo32/decapoda-research-llama-7B-hf\"\n",
    "model = llama.LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = llama.LlamaTokenizer.from_pretrained(model_name)\n",
    "print(model)\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model)\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(intervener):\n",
    "    def wrapped(*args, **kwargs):\n",
    "        return intervener(*args, **kwargs)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/like/miniconda3/envs/q-probe/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/like/miniconda3/envs/q-probe/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unintervened:\n",
      "('What to do in San Francisco? 10 things to do in San Francisco\\n'\n",
      " 'San Francisco is a city that is full of life and energy. There are so many '\n",
      " 'things to do in San Francisco that it can be hard to know where to start. '\n",
      " 'Here are 10 things to do in San Francisco that will help you make the most '\n",
      " 'of your time in this amazing city.\\n'\n",
      " '1. Visit the Golden Gate Bridge\\n'\n",
      " 'The Golden Gate Bridge is one of the most iconic landmarks in San Francisco. '\n",
      " 'It is a must-see for any visitor to the city. The bridge is 1.7 miles long '\n",
      " 'and 90 feet wide. It is made of steel and concrete and was completed in '\n",
      " '1937. The bridge is a popular spot for tourists to take photos and enjoy the '\n",
      " 'views of the city and the bay.\\n'\n",
      " '2. Take a cable car ride\\n'\n",
      " 'The cable cars are a San Francisco icon. They are a great way to see the '\n",
      " 'city and get around. The cable cars run on tracks that are laid on the '\n",
      " 'street. The cars are pulled by a cable that runs through the track. The cars '\n",
      " 'have two levels, with the upper level being open to the air. The cable cars '\n",
      " 'are a great way to see the city and get around.\\n'\n",
      " '3. Visit Alcatraz Island\\n'\n",
      " 'Alcatraz Island is')\n"
     ]
    }
   ],
   "source": [
    "q = \"What to do in San Francisco?\"\n",
    "prompt = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# run the uninterned forward pass\n",
    "outputs = model.generate(\n",
    "    prompt[\"input_ids\"],\n",
    "    max_new_tokens=256, \n",
    "    do_sample=False, \n",
    ")\n",
    "print(\"Unintervened:\")\n",
    "pprint(tokenizer.decode(outputs[0], skip_special_tokens=True))  # includes the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the feature for a certain goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = range(16, 32, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4096])\n",
      "torch.Size([1, 6, 4096])\n",
      "torch.Size([1, 6, 4096])\n",
      "torch.Size([1, 6, 4096])\n"
     ]
    }
   ],
   "source": [
    "q = \"The Tenderloin.\"\n",
    "prompt = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "class Collector():\n",
    "    def __init__(self):\n",
    "        self.collections = []\n",
    "    def __call__(self, b, s): \n",
    "        print(b.shape)\n",
    "        self.collections.append(b)\n",
    "        return b\n",
    "\n",
    "config = []\n",
    "collotors = []\n",
    "for layer in layers:\n",
    "    collotor = Collector()\n",
    "    collotors.append(collotor)\n",
    "    config.append({\n",
    "        \"component\": f\"model.layers[{layer}].self_attn.o_proj.input\",\n",
    "        \"intervention\": wrapper(collotor), \n",
    "        })\n",
    "\n",
    "pv_model = pv.IntervenableModel(config, model=model)\n",
    "\n",
    "_, intervened_outputs = pv_model(prompt)\n",
    "goals = [collotor.collections[0][0, 0] for collotor in collotors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/like/miniconda3/envs/q-probe/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/n/holylfs05/LABS/pfister_lab/Lab/coxfs01/pfister_lab2/Lab/like/miniconda3/envs/q-probe/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervened: \n",
      "('What to do in San Francisco? 10 things to do in San Francisco\\n'\n",
      " 'San Francisco is a city that is full of life and culture. It is a city that '\n",
      " 'is always changing and evolving, and there are always new things to see and '\n",
      " 'do. Here are 10 things to do in San Francisco:\\n'\n",
      " '1. Visit the Golden Gate Bridge: The Golden Gate Bridge is one of the most '\n",
      " 'iconic landmarks in the world, and it is definitely worth a visit. You can '\n",
      " 'walk or bike across the bridge, or you can take a ferry across the bay.\\n'\n",
      " '2. Visit Fisherman’s Wharf: Fisherman’s Wharf is a great place to visit, '\n",
      " 'especially if you are looking for seafood. There are also a lot of shops and '\n",
      " 'restaurants in the area, and it is a great place to people-watch.\\n'\n",
      " '3. Visit Chinatown: Chinatown is one of the largest Chinatowns in the world, '\n",
      " 'and it is definitely worth a visit. There are a lot of shops and restaurants '\n",
      " 'in the area, and it is a great place to experience Chinese culture.\\n'\n",
      " '4. Visit the Castro: The Castro is a neighborhood that is known for its LGBT '\n",
      " 'culture. It is a great place to visit, especially if you are looking for a '\n",
      " 'place to celebrate Pride.\\n'\n",
      " '5. Visit the Mission: The Mission is')\n"
     ]
    }
   ],
   "source": [
    "class Intervener():\n",
    "    def __init__(self, goal):\n",
    "        self.goal = goal\n",
    "        self.norm_intervene = 1.\n",
    "        self.alpha = 0.5\n",
    "    def __call__(self, b, s): \n",
    "        distance_to_goal = torch.norm(b[:, -1] - self.goal, dim=-1).item()\n",
    "        direction_to_goal = b[:, -1] - self.goal\n",
    "        # direction_to_goal = direction_to_goal / torch.norm(direction_to_goal, dim=-1, keepdim=True)\n",
    "        b[:, -1] = b[:, -1] - direction_to_goal * self.alpha\n",
    "        end_distance_to_goal = torch.norm(b[:, -1] - self.goal, dim=-1).item()\n",
    "        # print(distance_to_goal, \"--->\", end_distance_to_goal)\n",
    "        return b\n",
    "\n",
    "config = []\n",
    "interveners = []\n",
    "for layer, goal in zip(layers, goals):\n",
    "    intervener = Intervener(goal=goal)\n",
    "    interveners.append(intervener)\n",
    "    config.append({\n",
    "        \"component\": f\"model.layers[{layer}].mlp.output\",\n",
    "        \"intervention\": wrapper(intervener), \n",
    "        })\n",
    "\n",
    "\n",
    "pv_model = pv.IntervenableModel(config, model=model)\n",
    "\n",
    "q = \"What to do in San Francisco?\"\n",
    "prompt = tokenizer(q, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# run the intervened forward pass\n",
    "_, intervened_outputs = pv_model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=256, \n",
    "    do_sample=False, \n",
    ")\n",
    "print(\"Intervened: \")\n",
    "pprint(tokenizer.decode(intervened_outputs[0], skip_special_tokens=True))  # includes the prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyvene.models.intervenable_base.IntervenableModel"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pv_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iti_h100_kernel",
   "language": "python",
   "name": "iti_h100_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
